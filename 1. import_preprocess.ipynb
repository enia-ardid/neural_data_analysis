{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc85a109-4b48-4dd9-bc25-a74b358dd6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/enia/miniforge3/envs/jupyter_env/lib/python3.11/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/enia/miniforge3/envs/jupyter_env/lib/python3.11/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/enia/miniforge3/envs/jupyter_env/lib/python3.11/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/enia/miniforge3/envs/jupyter_env/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/enia/miniforge3/envs/jupyter_env/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4f9043a-187c-47e6-8007-bf46580533a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "plot_folder = os.path.expanduser(\"~/plots\")\n",
    "os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "data_folder = \"/dda2/enia\"  # Set full path to your data folder\n",
    "rat = \"jc320\"\n",
    "day = \"240924\"\n",
    "sessions = [\"training1\"]\n",
    "\n",
    "# ===> Parameters (edit as needed)\n",
    "def load_data(data_folder, rat, day, sessions):\n",
    "\n",
    "    import pandas as pd\n",
    "    base_path = os.path.join(data_folder, f\"{rat}-{day}\")\n",
    "    \n",
    "    # ----------- Helper: Load and concat files -----------\n",
    "    def load_and_concat(filenames, axis=0):\n",
    "        arrays = [np.loadtxt(f) for f in filenames]\n",
    "        return np.concatenate(arrays, axis=axis)\n",
    "    \n",
    "    def load_des_with_noise(des_path):\n",
    "        des = pd.read_csv(des_path, header=None, names=[\"type\"])\n",
    "        noise_df = pd.DataFrame({\"type\": [\"noise\", \"multiunit\"]})\n",
    "        des_full = pd.concat([noise_df, des], ignore_index=True)\n",
    "        return des_full\n",
    "    \n",
    "    def load_txt_list(path_list):\n",
    "        return [list(map(int, open(p).read().split())) for p in path_list]\n",
    "    \n",
    "    # ----------- Initialize containers -----------\n",
    "    res_all = []\n",
    "    clu_all = []\n",
    "    whl_all = []\n",
    "    whl_speed_all = []\n",
    "    reward_arms_all = []\n",
    "    all_arms_all = []\n",
    "    trials_all = []\n",
    "    \n",
    "    lwhl_raw_all = []\n",
    "    lwhl_all = []\n",
    "    lwhl_speed_all = []\n",
    "\n",
    "    # ----------- Gather all paths-----------\n",
    "    for session in sessions:\n",
    "        print(f\"üîÑ Loading session: {session}\")\n",
    "    \n",
    "        # .res and .clu\n",
    "        res_path = os.path.join(base_path, f\"{rat}-{day}_{session}.res\")\n",
    "        clu_path = os.path.join(base_path, f\"{rat}-{day}_{session}.clu\")\n",
    "        res = np.loadtxt(res_path, dtype=int)\n",
    "        clu = np.loadtxt(clu_path, dtype=int)[1:]  # skip first element\n",
    "        res_all.append(res)\n",
    "        clu_all.append(clu)\n",
    "    \n",
    "        # File paths only (headers will be preserved later if needed)\n",
    "        whl_all.append(os.path.join(base_path, f\"{rat}-{day}_{session}.whl\"))\n",
    "        whl_speed_all.append(os.path.join(base_path, f\"{rat}_{day}_{session}.speed\"))\n",
    "        trials_all.append(os.path.join(base_path, f\"{rat}_{day}_{session}.trials\"))\n",
    "        lwhl_raw_all.append(os.path.join(base_path, f\"{rat}_{day}_{session}.lwhl_raw\"))\n",
    "        lwhl_all.append(os.path.join(base_path, f\"{rat}_{day}_{session}.lwhl\"))\n",
    "        lwhl_speed_all.append(os.path.join(base_path, f\"{rat}_{day}_{session}.lwhl_speed\"))\n",
    "        reward_arms_all.append(os.path.join(base_path, f\"{rat}-{day}_{session}.reward_arms\"))\n",
    "        all_arms_all.append(os.path.join(base_path, f\"{rat}-{day}_{session}.all_arms\"))\n",
    "    \n",
    "    # ----------- Concatenate .res and .clu -----------\n",
    "    res = np.concatenate(res_all)\n",
    "    clu = np.concatenate(clu_all)\n",
    "    print(f\"res length: {len(res)}\")\n",
    "    print(f\"clu length: {len(clu)} (should match res)\")\n",
    "    \n",
    "    # ----------- Load .des with noise/multiunit types prepended -----------\n",
    "    des_path = os.path.join(base_path, f\"{rat}-{day}.des\")\n",
    "    putative_type = load_des_with_noise(des_path)\n",
    "    print(f\"des loaded: {len(putative_type)} total cluster types\")\n",
    "\n",
    "    # ----------- Load and concat .whl (no header) -----------\n",
    "    whl_list = [pd.read_csv(f, sep=r\"\\s+\", header=None) for f in whl_all]\n",
    "    whl = pd.concat(whl_list, ignore_index=True)\n",
    "    print(f\"whl shape: {whl.shape}\")\n",
    "    \n",
    "    # ----------- Load and concat .speed robustly (no header) -----------\n",
    "    speed_all = []\n",
    "    for speed_path in whl_speed_all:\n",
    "        try:\n",
    "            speed_df = pd.read_csv(speed_path, sep=None, engine=\"python\", header=None)\n",
    "            speed_col = speed_df.select_dtypes(include=[np.number]).iloc[:, 0]\n",
    "            speed_all.append(speed_col.values)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {speed_path}: {e}\")\n",
    "    \n",
    "    speed = np.concatenate(speed_all)\n",
    "    print(f\"speed length: {len(speed)}\")\n",
    "    print(f\"NaNs in speed: {np.isnan(speed).sum()} / {len(speed)}\")\n",
    "\n",
    "    # ----------- Load reward_arms and all_arms -----------\n",
    "    reward_arms = load_txt_list(reward_arms_all)\n",
    "    print(f\"Loaded {len(reward_arms)} reward_arms files\")\n",
    "    \n",
    "    all_arms = []\n",
    "    for file_path in all_arms_all:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    arms = [int(x) for x in line.strip().split()]\n",
    "                    all_arms.append(arms)\n",
    "    \n",
    "    print(f\"Loaded {len(all_arms)} trials from all_arms files\")\n",
    "    print(\"Example trial arms:\", all_arms[0])\n",
    "    \n",
    "    # ----------- Load and parse .trials (no header) -----------\n",
    "    trials_segments = []\n",
    "    for tpath in trials_all:\n",
    "        with open(tpath, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    times = [int(x) for x in line.split()]\n",
    "                    trials_segments.append(times)\n",
    "    \n",
    "    print(f\"Loaded {len(trials_segments)} trial segments from .trials\")\n",
    "    #print(\"‚è±Ô∏è  Example trial timestamps:\", trials_segments[0])\n",
    "\n",
    "    # ----------- Load .lwhl_raw, .lwhl and .lwhl_speed (WITH header) -----------\n",
    "    lwhl_raw_list = [pd.read_csv(f, sep=r\"\\s+\", header=0) for f in lwhl_raw_all]\n",
    "    lwhl_list = [pd.read_csv(f, sep=r\"\\s+\", header=0) for f in lwhl_all]\n",
    "    lwhl_speed_list = [pd.read_csv(f, sep=r\"\\s+\", header=0) for f in lwhl_speed_all]\n",
    "    \n",
    "    lwhl_raw   = pd.concat(lwhl_raw_list, ignore_index=True)\n",
    "    lwhl       = pd.concat(lwhl_list, ignore_index=True)\n",
    "    lwhl_speed = pd.concat(lwhl_speed_list, ignore_index=True)\n",
    "    \n",
    "    #print(f\"lwhl_raw shape: {lwhl_raw.shape}\")\n",
    "    print(f\"lwhl shape: {lwhl.shape}\")\n",
    "    print(f\"lwhl_speed shape: {lwhl_speed.shape}\")\n",
    "\n",
    "    return res, clu, putative_type, whl, speed, reward_arms, all_arms, trials_segments, lwhl_raw, lwhl, lwhl_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad39e23-c20b-4f90-9547-3d2d36725cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_speed(speed):\n",
    "    speed = np.nan_to_num(speed, nan=0.0)\n",
    "    speed = np.abs(speed)\n",
    "    print(f\"Cleaned speed: {len(speed)} values\")\n",
    "    return speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea827af-6cef-44a3-8ae4-56e36d58f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trials_events_armids(trials_segments):\n",
    "\n",
    "    trial_intervals = []\n",
    "    for segment in trials_segments:\n",
    "        if len(segment) >= 2:\n",
    "            trial_intervals.append([segment[0], segment[-1]])\n",
    "    \n",
    "    print(f\"Total trials: {len(trial_intervals)}\")\n",
    "    print(\"First trial:\", trial_intervals[0])\n",
    "\n",
    "    event_intervals = []\n",
    "    event_arm_ids = []\n",
    "    \n",
    "    for trial in trials_segments:\n",
    "        # Skip empty or incomplete\n",
    "        if len(trial) < 3:\n",
    "            continue\n",
    "    \n",
    "        # Remove start/end of trial to isolate events\n",
    "        events = trial[1:-1]\n",
    "    \n",
    "        # Each event has 4 timestamps: in, reward1, reward2, out\n",
    "        for i in range(0, len(events), 4):\n",
    "            try:\n",
    "                t_in, r1, r2, t_out = events[i:i+4]\n",
    "                event_intervals.append([t_in, t_out])\n",
    "            except ValueError:\n",
    "                continue  # skip incomplete events\n",
    "    \n",
    "    # Flatten all_arms and sync with event count\n",
    "    event_arm_ids = [arm for trial_arms in all_arms for arm in trial_arms]\n",
    "    \n",
    "    assert len(event_intervals) == len(event_arm_ids), \\\n",
    "        f\"Mismatch: {len(event_intervals)} events vs {len(event_arm_ids)} arm_ids\"\n",
    "    print(f\"Extracted {len(event_intervals)} events with arm IDs.\")\n",
    "\n",
    "    return trial_intervals, event_intervals, event_arm_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "345143c9-6950-4d46-a5a9-934a2c684d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_nested_list_to_ms(nested_list, sampling_rate_hz):\n",
    "    \"\"\"\n",
    "    Converts a list of lists of timestamps into samples on ms \n",
    "    \"\"\"\n",
    "    return [(np.array(vec) / sampling_rate_hz) * 1000 for vec in nested_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3498a8c6-eb90-46d5-b097-b266798116c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading session: training1\n",
      "res length: 16382116\n",
      "clu length: 16382116 (should match res)\n",
      "des loaded: 508 total cluster types\n",
      "whl shape: (295727, 2)\n",
      "speed length: 295727\n",
      "NaNs in speed: 39 / 295727\n",
      "Loaded 1 reward_arms files\n",
      "Loaded 39 trials from all_arms files\n",
      "Example trial arms: [4, 7, 1, 5, 1, 2, 8]\n",
      "Loaded 39 trial segments from .trials\n",
      "lwhl shape: (295727, 7)\n",
      "lwhl_speed shape: (295727, 4)\n",
      "Cleaned speed: 295727 values\n"
     ]
    }
   ],
   "source": [
    "res, clu, putative_type, whl, speed, reward_arms, all_arms, trials_segments, lwhl_raw, lwhl, lwhl_speed = load_data(data_folder, rat, day, sessions)\n",
    "\n",
    "speed = clean_speed(speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9573ad23-991b-4654-baec-8231a99b6d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trials: 39\n",
      "First trial: [0, 5793]\n",
      "Extracted 180 events with arm IDs.\n"
     ]
    }
   ],
   "source": [
    "trial_intervals, event_intervals, event_arm_ids = extract_trials_events_armids(trials_segments)\n",
    "\n",
    "trials_segments_ms = convert_nested_list_to_ms(trials_segments, sampling_rate_hz=39.0625)\n",
    "event_intervals_ms = convert_nested_list_to_ms(event_intervals, sampling_rate_hz=39.0625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0abfa392-d10f-4495-b226-a089f56b31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "def extract_event_trial_ids(lwhl: pd.DataFrame,\n",
    "    events_ms: List[Tuple[float, float]],\n",
    "    *,\n",
    "    sampling_rate_wheel: float = 39.0625,\n",
    "    trial_col: str = \"trial_id\", min_fraction: float = 0.5,\n",
    "    fill_on_empty: bool = True,) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Devuelve un vector (n_events,) con el trial_id 'modo' (m√°s frecuente) dentro de cada intervalo [start_ms, end_ms).\n",
    "    - lwhl.index son √≠ndices de muestreo (0..N-1) a 39.0625 Hz (25.6 ms/ muestra).\n",
    "    - Si un evento cae entre dos trials, toma la moda. Si la moda < min_fraction del intervalo, marca -1 (ambiguo).\n",
    "    - Si no hay muestras dentro del intervalo y fill_on_empty=True, usa el 'trial_ID' del sample m√°s cercano al inicio; si no, -1.\n",
    "\n",
    "    Consejos:\n",
    "    - Si tu columna tiene NaNs (p. ej. en el centro), la moda se calcula ignorando NaNs.\n",
    "    \"\"\"\n",
    "    assert trial_col in lwhl.columns, f\"Columna '{trial_col}' no encontrada en lwhl\"\n",
    "    ms_per_sample = 1000.0 / float(sampling_rate_wheel)\n",
    "    n = len(lwhl)\n",
    "\n",
    "    trial_ids = np.full(len(events_ms), -1, dtype=int)\n",
    "\n",
    "    # vector numpy del trial_ID (puede contener NaNs)\n",
    "    col = lwhl[trial_col].to_numpy()\n",
    "\n",
    "    for i, (start_ms, end_ms) in enumerate(events_ms):\n",
    "        # √≠ndices [start_idx, end_idx) en muestras\n",
    "        start_idx = int(np.floor(start_ms / ms_per_sample))\n",
    "        end_idx   = int(np.ceil (end_ms   / ms_per_sample))\n",
    "\n",
    "        # clamp\n",
    "        start_idx = max(0, min(start_idx, n-1))\n",
    "        end_idx   = max(start_idx+1, min(end_idx, n))  # end exclusivo\n",
    "\n",
    "        seg = col[start_idx:end_idx]\n",
    "\n",
    "        # ignora NaNs\n",
    "        if seg.dtype.kind in \"fF\" or np.issubdtype(seg.dtype, np.floating):\n",
    "            seg_valid = seg[~np.isnan(seg)]\n",
    "        else:\n",
    "            seg_valid = seg\n",
    "\n",
    "        if seg_valid.size == 0:\n",
    "            if fill_on_empty:\n",
    "                # usa el sample m√°s cercano al inicio\n",
    "                near_idx = int(np.clip(round(start_ms / ms_per_sample), 0, n-1))\n",
    "                val = col[near_idx]\n",
    "                if (isinstance(val, (float, np.floating)) and np.isnan(val)):\n",
    "                    trial_ids[i] = -1\n",
    "                else:\n",
    "                    trial_ids[i] = int(val)\n",
    "            else:\n",
    "                trial_ids[i] = -1\n",
    "            continue\n",
    "\n",
    "        # moda + fracci√≥n\n",
    "        vals, counts = np.unique(seg_valid.astype(int), return_counts=True)\n",
    "        j = int(np.argmax(counts))\n",
    "        mode_val, mode_cnt = int(vals[j]), int(counts[j])\n",
    "        frac = mode_cnt / float(seg_valid.size)\n",
    "\n",
    "        trial_ids[i] = mode_val if frac >= float(min_fraction) else -1\n",
    "\n",
    "    return trial_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dc737ef-3f99-412f-9718-f870642982bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_ids_all = extract_event_trial_ids(lwhl, event_intervals_ms, sampling_rate_wheel=39.0625)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "rewarded_arms_session = np.array(reward_arms, dtype=int)      \n",
    "rewarded_mask = np.isin(event_arm_ids, rewarded_arms_session)\n",
    "\n",
    "idx_rewards = np.where(rewarded_mask)[0]\n",
    "trial_ids_rewarded = trial_ids_all[idx_rewards]\n",
    "\n",
    "pre_trial_ids_all = []\n",
    "pre_trial_ids_rewarded = []\n",
    "\n",
    "for i in range(1, len(trial_ids_all)):\n",
    "    current_trial = trial_ids_all[i]\n",
    "    previous_trial = trial_ids_all[i-1]\n",
    "    pre_trial_ids_all.append(current_trial) # label of the trial of the NEXT event\n",
    "\n",
    "for i in range(1, len(trial_ids_rewarded)):\n",
    "    current_trial = trial_ids_rewarded[i]\n",
    "    previous_trial = trial_ids_rewarded[i-1]\n",
    "    pre_trial_ids_rewarded.append(current_trial) # label of the trial of the NEXT event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18f2451f-f082-4951-a5f4-5c1e0c31338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as raw_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"raw_data.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"res\": res,\n",
    "            \"clu\": clu,\n",
    "            \"putative_type\": putative_type,\n",
    "            \"whl\": whl,\n",
    "            \"speed\": speed,\n",
    "            \"reward_arms\": reward_arms,\n",
    "            \"all_arms\": all_arms,\n",
    "            \"lwhl\": lwhl,\n",
    "            \"trials_segments_ms\": trials_segments_ms,\n",
    "            \"event_intervals_ms\": event_intervals_ms,\n",
    "            \"event_arm_ids\": event_arm_ids,\n",
    "            \"trial_ids_all\": trial_ids_all,\n",
    "            \"trial_ids_rewarded\":trial_ids_rewarded}, f)\n",
    "    \n",
    "print(\"Saved as raw_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a737bdf-75d0-48d1-8ef2-74081bebf364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
